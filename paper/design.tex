\section{System Design}\label{sec:design}

This section describes the design of our system.  We start out with the
construction of data aggregation trees.  We then discuss mechanism to compress
messages accumulated at internal tree nodes.  This is followed by discussion on
more details of the system.

\subsection{Building Data Aggregation Trees}

We use \emph{racks} as basic units to build data aggregation trees.  Within
each rack, one machine is selected to be in charge of all communications with
other racks.  We call such machines \emph{hubs} of their racks.  These hubs
serve as nodes in data aggregation trees. All the non-hub machines in each rack
simply send messages to their hub, making the structure across all the machines
still a tree.  However, these non-hub nodes are ignored in our discussion of
data aggregation trees.  Since datacenter network topology is relatively
static, we assume that it is not necessary to explicitly maintain rack
membership.

The \emph{fanout} of each node in a tree is set as a configuration parameter.
Larger fanout value result in fewer hops from a leaf node to the root.  With a
particular fanout $k$, we always construct complete $k$-ary trees since it
gives us two benefits.  First, the hop difference from any leaf node to root is
minimized.  In addition, the complete tree gives us the most leaf nodes, which
is necessary for building multiple independent trees.

We first discuss how to build only one data aggregation tree, then we
generalize to build multiple trees over the same set of racks.

Note that the rack containing the receiver is not included in the tree-building
process.  Instead, it receives messages from the roots of all the trees.
Section~\ref{sec:design:ring} provides further discussions.

\subsubsection*{Building One Tree}

The algorithm to build a complete $k$-ary tree works as follows.  Each rack is
assigned a unique number as its identifier.  We designate one rack as the root,
and let the root generate a random permutation of identifiers of all the other
racks.  This permutation serves as the level-order traversal of the tree except
for the root.  With complete $k$-ary trees, level-order traversal uniquely
describes the tree structure.  Starting from the root, each rack sends the
permutation to all its children racks.  This process is completed when every
rack has learned the tree structure.

\subsubsection*{Building Multiple Trees}

The goal of having multiple trees is to improve resiliency and bandwidth
utilization.  Therefore, we require that each rack serves as internal nodes in
at most one of the trees.  This requirement ensures that as long as there are
more than one trees, a single rack failure never blocks another rack from
reaching at least one root.  However, it also puts a limit on the number of
trees can be built.  For a complete $k$-ary tree with $n$ racks, the number of
internal racks is $n_i = \left\lfloor\frac{n + k - 2}{k}\right\rfloor$.  With
$k = 2$, $n_i = 2 * \left\lfloor\frac{n}{2}\right\rfloor$, so there are always
enough nodes to build two independent trees.  But when $k > 2$, we can show
that there are cases where at most $k - 1$ trees can be built without violating
the constraint that any node serves as internal node at most once.  [[TODO:
Attach a proof]]

Based on the above discussion, the system builds two trees when fanout is $2$,
and $k - 1$ trees with larger fanout $k$.  The tree building algorithm starts
by selecting $t * n_i$ racks at random, where $t$ is the number of trees.
These racks are divided into $t$ groups to serve as internal nodes of the
trees.  To build each particular tree, we generate a random permutation of all
its internal nodes, and a random permutation of all other nodes, i.e. the
leaves.  The two permutation together completes the level-order traversal of a
tree.  This tree structure is propagated through all the nodes as in the case
of building a single tree described previously.

\subsubsection*{Machines within a Rack}

So far, all the machines in a rack sends messages directly to the hub.  This is
feasible when the number of the machines in a rack is small (a typical
datacenter rack has up to $24$ nodes~\cite{ALV08}).  But should this become a
problem, we can organize machines within each rack into tree structures with
the same approach.

\subsection{Data Compaction at Internal Nodes}

A node forwards all the messages generated by itself and other nodes within its
subtree to its parent node.  By default, a message is forwarded immediately
after it is received.  Assume that all the machines generate messages at the
same rate, the forwarding load grows exponentially up the tree.

We provide a mechanism to reduce message load at internal tree nodes based on
the following observation.  In many applications, messages could make other
ones obsolete, and it is not necessary for the system to deliver obsolete
messages.  For example, in web caching, if a message says a set of objects are
being updated, all other messages updating objects within that set become
redundant.  Similarly in smart home sensor networks, if a message updates the
reading of a particular sensor, all prior messages about readings of this
same sensor are now obsolete.

The system makes use of such observations by delaying message forwarding at each
node.  This allows nodes to accumulate messages and discard redundant ones.
Specifically, the application specifies the maximum delay a message could
afford before it is delivered to the root.  The amount of delay is distributed
onto nodes at each level of the tree.  Instead of forwarding, each node buffers
messages it has received.  A background process scans through the message
buffer periodically to mark messages as obsolete based on contents in the
message buffer.  Messages are forwarded up the tree only if they have not been
marked as obsolete when the timeout is reached.

It is up to applications to decide the amount of delay, and how it is
distributed among the tree nodes.  Section~\ref{sec:eval} discusses the effect
of changing both total message delay and the distribution policy.

\subsection{Bandwidth Allocation}

In real world applications, it is often the case that machines generate
messages at different rates.  If a fraction of the machines generate too many
messages, others may be starved.  We provide an option to ensure fairness
across machines with bandwidth quotas.

Bandwidth allocation is done in two phases.  The first phase collects each
machine's desired bandwidth from bottom up.  Each individual machine in the
system request a desired qouta to its parent node in the tree.  The tree nodes
aggregates all the quota requests from its subtree, and propagates this
information upwards.  In the end, all the nodes in the tree have the
information of the total desired bandwidth for its entire subtree.

The second phase allocates bandwidth from top down.  The total available
bandwidth, $B$, is the inbound bandwidth at the root node.  Suppose the system
has $n$ machines in total, and the rack contains the root has $n_0$ nodes.
Also suppose that the root has $k$ children, and the totol number of machines
in each subtree is $n_1, n_2, ..., n_k$.  If nodes in $n_i$, if the total
desired bandwidth $D_i$ does not exceed its fair share, $\frac{n_i}{n}B$, the
root simply allocates $D_i$.  Otherwise, $\frac{n_i}{n}B$ is allocated to this
subtree.  If there is still bandwidth available, and at least one group of
nodes got less than their desired quota, this process is repeated for the
remaining bandwidth on those set of nodes.  After the root finishes, it sends
the bandwidth quota $B_i$ to each group, and the same allocation process
repeats within their subtrees.

\subsection*{Adjust Quotas with Data Compaction}

Each machine requests bandwidth based on the rate it generates messages.  With
data compaction, the actual bandwidth usage is lower than the amount requested
when redundant messages are discarded.  To make use of this extra bandwidth, we
make two changes to the bandwidth allocation process.

\textbf{Quota Amplification}.  Each node keeps track of the ratio between its
inbound and outbound traffic, $r$.  Inbound traffic also includes messages
generated by the node itself and all the machines in its rack.  Suppose the
quota it gets from its parent is $B$, then we set the quota it can allocate to
all its subtrees $B' = rB$.  

\textbf{Adaptive Reallocation}.  Each node also monitors the actual bandwidth
usage of all its subtrees, and periodically make adjustments by reallocating
the quotas.  The adjustment works as follows.  Starting from the root, a node
checks all the groups $n_i$, and adjust the bandwidth quota to its currect
usage if any group's usage is less than the allocated amount $B_i$.  It then
checks for all the groups that the current quota is less than what it
requested, and allocate the reclaimed bandwidth to these groups proportionally
to their number of machines.

\subsection{Prioritization}

The tolerance of delay vary among different types of messages.  Some urgent
messages could not even afford any delay.  We address this issue by supporting
message prioritizations.  Applications are allowed to specify multiple priority
levels.  Each priority level sets the total message delay independently.  A
delay of zero means messages in this level are always forwarded immediately.
To support prioritization, we implement message buffers at tree nodes using
priority queues.  Messages are ordered by their timeout calculated based on
their priority levels.

\subsection{Maintain Tree Structures upon Failures}

The multiple tree mechanism provides resiliency in the face of failures.  This
section shows how the structure repair itself after failures.  We consider two
types of failures:

\begin{itemize}
\item \textbf{Single machine failure} occurs when an individual machine goes down.
\item \textbf{Rack failure} occurs when all the machines in the same rack are not
responsive.  It may be caused by failure of the top-of-rack switch,
maintenance, or power outage of the rack.
\end{itemize}

A single machine failure does not affect the system if that machine is not the
hub of its rack.  If the hub of a rack fails, a new hub is selected at random
from other machines in the rack.

In the case of a rack failure, the tree structures need to be repaired.  Two
scenarios apply here.  For the trees in which this rack is a leaf, we swap its
position with the last rack in the tree's level-order traversal, and then
remove the failed rack.  This guarantees the reconfigured tree is still a
complete $k$-ary tree.  For the tree in which this rack is an internal node,
we replace this failed rack with a rack that serves as leaf nodes in all the
trees.  [[TODO: Attach a proof]]

\subsection{Fault Tolerance and Load Balancing at Receiver}
\label{sec:design:ring}

The receiver itself could be made fault tolerant with replication methods such
as Primary/Backup~\cite{BMST93} or Chain Replication~\cite{vRS04}.  However,
the load across replicas are not balanced in such approaches---the primary
assumes the overhead as coordinator in Primary/Backup, whereas the tail handles
all the read requests in Chain Replication.  We propose a technique called
\emph{Ring Replication} to provide better load balancing and additional
robustness for the multiple tree structure. 

Ring Replication is derived from Chain Replication.  We set up three replicas
of the receiver, and organize them into a ring.  Each replica can receive
messages from the sender network.  Upon receiving a message, the replica stores
the message, and forward it to the next replica in the ring.  The message is
reliably stored in the ring when all the three replicas have stored it.  It is
effectively three chains, with each replica assumes a different role in each
chain.  Since messages are not ordered, each chain can form its own stream of
messages without coordination with other chains.

Since all the replicas can receive messages independently, it is desirable to
distribute the workload evenly across them.  We provide two mechanisms to
achieve this.  The first approach divides all the non-receiver racks evenly
into three sets.  Each set builds its own trees and chooses a replica as outlet
of their messages.  The second approach deals with multiple trees built from
the same set of racks.  It connects the root of each tree to a different receiver
replica using round robin.  An application can combine the two approaches, so
that the depth of each tree is smaller, and the impact of a failed receiver
replica is minimized.

In typical datacenters, machines are homogeneous.  However, we argue that it is
beneficial to use dedicated high bandwidth machines for the receiver replicas.
As we show in Section~\ref{sec:eval}, although we are able to improve load
balancing across the nodes, the root of each aggregation tree still receives
higher load than other nodes.  Having receiver replicas as the outlet of
multiple trees works best when there is extra bandwidth at the receivers.
Since there is only a small number of receiver replicas ($3$ in our
configurations), it is possible to just install more NICs on the receiver
replica, and use spare ports of its top-of-rack switch for extra bandwidth.

